# -*- coding: utf-8 -*-
"""Unet-ResNet50-Grad-CAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cc1IGZGsu5WagcwUEd_Z7jWbO1PvodNQ
"""

!pip install segmentation-models tensorflow albumentations scikit-learn

# @title Unet
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, jaccard_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
import albumentations as A

# ==== Load BUSI Dataset ====
def load_busi_data(base_path, img_size=(256,256)):
    X, Y = [], []
    for cls in ['benign', 'malignant']:
        path = os.path.join(base_path, cls)
        files = glob(os.path.join(path, "*.png"))
        for img_path in files:
            if '_mask' in img_path:
                continue
            mask_path = img_path.replace(".png", "_mask.png")
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if img is None or mask is None:
                continue
            img = cv2.resize(img, img_size) / 255.0
            mask = cv2.resize(mask, img_size) / 255.0
            mask = (mask > 0.5).astype(np.uint8)
            X.append(np.expand_dims(img, axis=-1))
            Y.append(np.expand_dims(mask, axis=-1))
    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.uint8)


data_path = "/content/Dataset_BUSI_with_GT"
X, Y = load_busi_data(data_path)

# ==== Data Augmentation ====
def augment(X, Y, n_aug=2):
    transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.2),
        A.Rotate(limit=20, p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),
        A.ElasticTransform(alpha=1, sigma=50, p=0.3),
        A.GridDistortion(p=0.3),
    ])

    X_aug, Y_aug = [], []
    for i in range(len(X)):
        for _ in range(n_aug):
            img_uint8 = (X[i].squeeze()*255).astype(np.uint8)
            mask_uint8 = (Y[i].squeeze()*255).astype(np.uint8)
            augmented = transform(image=img_uint8, mask=mask_uint8)
            img_aug = augmented['image'].astype(np.float32)/255.0
            mask_aug = (augmented['mask'] > 127).astype(np.uint8)
            X_aug.append(np.expand_dims(img_aug, -1))
            Y_aug.append(np.expand_dims(mask_aug, -1))
    return np.array(X_aug, dtype=np.float32), np.array(Y_aug, dtype=np.uint8)

X_aug, Y_aug = augment(X, Y)
X = np.concatenate([X, X_aug])
Y = np.concatenate([Y, Y_aug])

# ==== Train/Validation/Test Split ====

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

# ==== Advanced U-Net Model ====
def conv_block(x, filters, dropout=0.0):
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    if dropout > 0:
        x = layers.Dropout(dropout)(x)
    return x

def encoder_block(x, filters, dropout=0.0):
    c = conv_block(x, filters, dropout)
    p = layers.MaxPooling2D(2)(c)
    return c, p

def decoder_block(x, skip, filters):
    us = layers.Conv2DTranspose(filters, 2, strides=2, padding='same')(x)
    concat = layers.concatenate([us, skip])
    c = conv_block(concat, filters)
    return c

def build_unet(input_shape=(256,256,1)):
    inputs = layers.Input(input_shape)
    c1, p1 = encoder_block(inputs, 64)
    c2, p2 = encoder_block(p1, 128)
    c3, p3 = encoder_block(p2, 256, dropout=0.1)
    c4, p4 = encoder_block(p3, 512, dropout=0.2)
    bn = conv_block(p4, 1024, dropout=0.3)
    d1 = decoder_block(bn, c4, 512)
    d2 = decoder_block(d1, c3, 256)
    d3 = decoder_block(d2, c2, 128)
    d4 = decoder_block(d3, c1, 64)
    outputs = layers.Conv2D(1, 1, activation='sigmoid')(d4)
    return models.Model(inputs, outputs)


def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coef(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)
    return bce + dice_loss(y_true, y_pred)

def iou_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection
    return (intersection + smooth) / (union + smooth)

# ==== Compile Model ====
model = build_unet()
model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef, iou_coef, 'accuracy'])

# ==== Callbacks ====
callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6),
    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),
    ModelCheckpoint("best_unet_model.h5", monitor='val_loss', save_best_only=True, verbose=1)
]

# ==== Train Model ====
history = model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=50,
    batch_size=16,
    callbacks=callbacks,
    verbose=2
)

# ==== Predict on Test Set ====
y_pred = model.predict(X_test)
y_pred_bin = (y_pred > 0.5).astype(np.uint8)

# ==== Evaluate Results ====
dice_scores, iou_scores, prec_scores, rec_scores = [], [], [], []

for i in range(len(Y_test)):
    gt = Y_test[i].squeeze()   # (H,W)
    pr = y_pred_bin[i].squeeze()
    if np.sum(gt) == 0 and np.sum(pr) == 0:
        continue
    gt_flat = gt.flatten()
    pr_flat = pr.flatten()
    dice = (2*np.sum(gt*pr)+1e-6)/(np.sum(gt)+np.sum(pr)+1e-6)
    iou = jaccard_score(gt_flat, pr_flat, zero_division=0)
    prec = precision_score(gt_flat, pr_flat, zero_division=0)
    rec = recall_score(gt_flat, pr_flat, zero_division=0)
    dice_scores.append(dice)
    iou_scores.append(iou)
    prec_scores.append(prec)
    rec_scores.append(rec)

print(f"âœ… Final Evaluation:")
print(f"Dice Coefficient: {np.mean(dice_scores)*100:.2f}%")
print(f"IoU Score: {np.mean(iou_scores)*100:.2f}%")
print(f"Precision: {np.mean(prec_scores):.2f}")
print(f"Recall: {np.mean(rec_scores):.2f}")

# ==== Visualize Predictions ====
def show_prediction(idx):
    plt.figure(figsize=(12,4))
    plt.subplot(1,4,1)
    plt.imshow(X_test[idx].squeeze(), cmap='gray')
    plt.title('Original')
    plt.axis('off')
    plt.subplot(1,4,2)
    plt.imshow(Y_test[idx].squeeze(), cmap='gray')
    plt.title('Ground Truth')
    plt.axis('off')
    plt.subplot(1,4,3)
    plt.imshow(y_pred[idx].squeeze(), cmap='gray')
    plt.title('Raw Prediction')
    plt.axis('off')
    plt.subplot(1,4,4)
    plt.imshow(y_pred_bin[idx].squeeze(), cmap='gray')
    plt.title('Binary Mask')
    plt.axis('off')
    plt.show()

# Display 5 sample predictions
for i in range(5):
    show_prediction(i)

import zipfile
import os

zip_path = "/content/BUSI.zip"
extract_path = "/content/"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extracted files:")
print(os.listdir(extract_path))
print(os.listdir(os.path.join(extract_path, "Dataset_BUSI_with_GT")))

# @title ResNet50 classification
import os
import cv2
import numpy as np
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.utils import class_weight

# === Load BUSI dataset for classification ===
def load_busi_classification_data(base_path, img_size=(256, 256)):
    X, Y = [], []
    for cls in ['benign', 'malignant']:
        label = 0 if cls == 'benign' else 1
        path = os.path.join(base_path, cls)
        files = sorted(glob(os.path.join(path, '*.png')))
        for img_path in files:
            if '_mask' in img_path:
                continue
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                continue
            img = cv2.resize(img, img_size) / 255.0
            X.append(np.expand_dims(img, -1))
            Y.append(label)
    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.uint8)


data_path = "/content/Dataset_BUSI_with_GT"
X, Y = load_busi_classification_data(data_path)

# === Simple Data Augmentation (flip operations) ===
X_aug, Y_aug = [], []
for i in range(len(X)):
    X_aug.append(X[i])
    Y_aug.append(Y[i])
    X_aug.append(np.flip(X[i], axis=1))
    Y_aug.append(Y[i])
    X_aug.append(np.flip(X[i], axis=0))
    Y_aug.append(Y[i])
X = np.array(X_aug)
Y = np.array(Y_aug)

# === Train/Validation/Test Split ===
X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Convert grayscale images to 3-channel (required by ResNet)
X_train_rgb = np.repeat(X_train, 3, axis=-1)
X_val_rgb = np.repeat(X_val, 3, axis=-1)
X_test_rgb = np.repeat(X_test, 3, axis=-1)

# === Define ResNet50-based Model ===
base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(256, 256, 3))
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])

# Compute class weights to address class imbalance
weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = {i: weights[i] for i in range(len(weights))}

# === Train Model ===
history = model.fit(
    X_train_rgb, y_train,
    validation_data=(X_val_rgb, y_val),
    epochs=20,
    batch_size=16,
    class_weight=class_weights,
    verbose=2
)


# === Evaluate on Test Set ===
y_pred_prob = model.predict(X_test_rgb).flatten()
y_pred = (y_pred_prob > 0.5).astype(int)

print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))

print("ðŸ”¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"ðŸ“ˆ AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}")

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# --- Compute confusion matrix ---
cm = confusion_matrix(y_test, y_pred)

# --- Plot confusion matrix heatmap ---
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - ResNet50")
plt.tight_layout()
plt.savefig("confusion_matrix_cls.png", dpi=300)  # Saved for inclusion in dissertation
plt.show()

from sklearn.metrics import roc_curve, auc

# --- Compute FPR, TPR, and AUC ---
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# --- Plot ROC curve ---
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC = {:.2f}'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.savefig("roc_curve_cls.png", dpi=300)
plt.show()

# @title Grad-CAM for ResNet50 Classification
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import cv2

# --- Function to generate Grad-CAM ---
def get_gradcam_heatmap(model, image, class_index=None, layer_name=None):
    """
    Generates Grad-CAM heatmap for a single input image.
    Args:
        model: Trained Keras model
        image: Input image (shape: (256, 256, 3))
        class_index: Class to visualize. If None, uses predicted class.
        layer_name: Convolutional layer to target (default = last conv layer)

    Returns:
        heatmap (numpy array): Grad-CAM heatmap (256x256)
    """
    if layer_name is None:
        layer_name = [layer.name for layer in model.layers if 'conv' in layer.name][-1]

    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(layer_name).output, model.output]
    )

    img_tensor = tf.expand_dims(image, axis=0)  # (1, 256, 256, 3)

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_tensor)
        if class_index is None:
            class_index = tf.argmax(predictions[0])
        output = predictions[:, class_index]

    grads = tape.gradient(output, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))  # (channels,)
    conv_outputs = conv_outputs[0]

    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)

    heatmap = np.maximum(heatmap, 0)
    heatmap /= tf.reduce_max(heatmap)
    heatmap = heatmap.numpy()
    heatmap = cv2.resize(heatmap, (256, 256))

    return heatmap

# --- Function to overlay heatmap on original image ---
def overlay_heatmap(original_img, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET):
    """
    Overlays the Grad-CAM heatmap on the original image.
    """
    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap), colormap)
    overlay = cv2.addWeighted(np.uint8(original_img * 255), 1 - alpha, heatmap_color, alpha, 0)
    return overlay

# --- Run Grad-CAM for selected test samples ---
num_samples = 5  # How many test images to visualize
indices = np.random.choice(len(X_test_rgb), num_samples, replace=False)

plt.figure(figsize=(12, num_samples * 3))
for i, idx in enumerate(indices):
    img = X_test_rgb[idx]
    label = y_test[idx]
    prediction = model.predict(np.expand_dims(img, axis=0))[0][0]
    predicted_class = int(prediction > 0.5)

    heatmap = get_gradcam_heatmap(model, img)
    overlay = overlay_heatmap(img, heatmap)

    # Display
    plt.subplot(num_samples, 3, i * 3 + 1)
    plt.imshow(img[..., 0], cmap='gray')
    plt.title("Original")
    plt.axis('off')

    plt.subplot(num_samples, 3, i * 3 + 2)
    plt.imshow(heatmap, cmap='jet')
    plt.title("Grad-CAM Heatmap")
    plt.axis('off')

    plt.subplot(num_samples, 3, i * 3 + 3)
    plt.imshow(overlay)
    plt.title(f"Overlay (Pred: {predicted_class}, True: {label})")
    plt.axis('off')

plt.tight_layout()
plt.savefig("figures/gradcam_examples.png", dpi=300)
plt.show()
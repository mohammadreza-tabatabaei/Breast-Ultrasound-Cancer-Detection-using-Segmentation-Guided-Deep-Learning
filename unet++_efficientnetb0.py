# -*- coding: utf-8 -*-
"""Unet++ - EfficientNetB0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cz9ZUIG3-Yk9PRo0LwOX8EOQc8zteC2Q
"""

!pip uninstall -y numpy scipy scikit-learn tensorflow

!pip install numpy==1.23.5 scipy==1.10.1 scikit-learn==1.2.2 tensorflow==2.12.0 --quiet

!pip install segmentation-models tensorflow albumentations scikit-learn

!pip uninstall -y tensorflow
!pip install tensorflow==2.7.0
!pip install segmentation-models albumentations scikit-learn

#unet++
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, jaccard_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
import albumentations as A

# === Load BUSI Dataset ===
def load_busi_data(base_path, img_size=(256, 256)):
    X, Y = [], []
    for cls in ['benign', 'malignant']:
        path = os.path.join(base_path, cls)
        files = glob(os.path.join(path, '*.png'))
        for img_path in files:
            if '_mask' in img_path:
                continue
            mask_path = img_path.replace('.png', '_mask.png')
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if img is None or mask is None:
                continue
            img = cv2.resize(img, img_size) / 255.0
            mask = cv2.resize(mask, img_size) / 255.0
            mask = (mask > 0.5).astype(np.uint8)
            X.append(np.expand_dims(img, -1))
            Y.append(np.expand_dims(mask, -1))
    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.uint8)

# === Data Augmentation ===
def augment(X, Y, n_aug=2):
    transform = A.Compose([
        A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.2), A.Rotate(limit=20, p=0.5),
        A.RandomBrightnessContrast(p=0.5), A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),
        A.ElasticTransform(alpha=1, sigma=50, p=0.3), A.GridDistortion(p=0.3)
    ])
    X_aug, Y_aug = [], []
    for i in range(len(X)):
        for _ in range(n_aug):
            img = (X[i].squeeze() * 255).astype(np.uint8)
            mask = (Y[i].squeeze() * 255).astype(np.uint8)
            augmented = transform(image=img, mask=mask)
            img_aug = augmented['image'].astype(np.float32) / 255.0
            mask_aug = (augmented['mask'] > 127).astype(np.uint8)
            X_aug.append(np.expand_dims(img_aug, -1))
            Y_aug.append(np.expand_dims(mask_aug, -1))
    return np.array(X_aug), np.array(Y_aug)

# === Build U-Net++ ===
def conv_block(x, filters):
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    return x

def build_unetpp(input_shape=(256, 256, 1)):
    inputs = layers.Input(input_shape)
    x0_0 = conv_block(inputs, 64)
    x1_0 = conv_block(layers.MaxPooling2D()(x0_0), 128)
    x2_0 = conv_block(layers.MaxPooling2D()(x1_0), 256)
    x3_0 = conv_block(layers.MaxPooling2D()(x2_0), 512)
    x4_0 = conv_block(layers.MaxPooling2D()(x3_0), 1024)
    x3_1 = conv_block(layers.concatenate([x3_0, layers.UpSampling2D()(x4_0)]), 512)
    x2_2 = conv_block(layers.concatenate([x2_0, layers.UpSampling2D()(x3_1)]), 256)
    x1_3 = conv_block(layers.concatenate([x1_0, layers.UpSampling2D()(x2_2)]), 128)
    x0_4 = conv_block(layers.concatenate([x0_0, layers.UpSampling2D()(x1_3)]), 64)
    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x0_4)
    return models.Model(inputs, outputs)

# === Loss & Metrics ===
def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coef(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)
    return bce + dice_loss(y_true, y_pred)

def iou_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection
    return (intersection + smooth) / (union + smooth)

# === Training ===
data_path = "/content/Dataset_BUSI_with_GT"
X, Y = load_busi_data(data_path)
X_aug, Y_aug = augment(X, Y)
X = np.concatenate([X, X_aug])
Y = np.concatenate([Y, Y_aug])
X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

model = build_unetpp()
model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef, iou_coef, 'accuracy'])

callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6),
    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),
    ModelCheckpoint("best_unetpp_model.h5", monitor='val_loss', save_best_only=True, verbose=1)
]

history = model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=50,
    batch_size=16,
    callbacks=callbacks,
    verbose=2
)

# === Evaluation ===
y_pred = model.predict(X_test)
y_pred_bin = (y_pred > 0.5).astype(np.uint8)
dice_scores = []
iou_scores = []
prec_scores = []
rec_scores = []
for i in range(len(Y_test)):
    gt = Y_test[i].squeeze()
    pr = y_pred_bin[i].squeeze()
    if np.sum(gt) == 0 and np.sum(pr) == 0:
        continue
    gt_flat = gt.flatten()
    pr_flat = pr.flatten()
    dice = (2 * np.sum(gt * pr) + 1e-6) / (np.sum(gt) + np.sum(pr) + 1e-6)
    iou = jaccard_score(gt_flat, pr_flat, zero_division=0)
    prec = precision_score(gt_flat, pr_flat, zero_division=0)
    rec = recall_score(gt_flat, pr_flat, zero_division=0)
    dice_scores.append(dice)
    iou_scores.append(iou)
    prec_scores.append(prec)
    rec_scores.append(rec)

print(f"\nâœ… Final Evaluation (U-Net++):")
print(f"Dice Coefficient: {np.mean(dice_scores) * 100:.2f}%")
print(f"IoU Score: {np.mean(iou_scores) * 100:.2f}%")
print(f"Precision: {np.mean(prec_scores):.2f}")
print(f"Recall: {np.mean(rec_scores):.2f}")
# === Visualize Predictions ===
def plot_prediction(index):
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 4, 1)
    plt.imshow(X_test[index].squeeze(), cmap='gray')
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 4, 2)
    plt.imshow(Y_test[index].squeeze(), cmap='gray')
    plt.title("Ground Truth")
    plt.axis("off")

    plt.subplot(1, 4, 3)
    plt.imshow(y_pred[index].squeeze(), cmap='gray')
    plt.title("Raw Prediction")
    plt.axis("off")

    plt.subplot(1, 4, 4)
    plt.imshow(y_pred_bin[index].squeeze(), cmap='gray')
    plt.title("Binary Mask")
    plt.axis("off")

    plt.tight_layout()
    plt.show()
# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
for i in range(3):
    plot_prediction(i)

# @title EfficientnetB0
import os
import cv2
import numpy as np
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.utils import class_weight
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# --- Load BUSI dataset for classification ---
def load_busi_classification_data(base_path, img_size=(256, 256)):
    X, Y = [], []
    for cls in ['benign', 'malignant']:
        label = 0 if cls == 'benign' else 1
        path = os.path.join(base_path, cls)
        files = sorted(glob(os.path.join(path, '*.png')))
        for img_path in files:
            if '_mask' in img_path:
                continue
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                continue
            img = cv2.resize(img, img_size) / 255.0
            X.append(np.expand_dims(img, -1))
            Y.append(label)
    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.uint8)

# Path to dataset
data_path = "/content/Dataset_BUSI_with_GT"
X, Y = load_busi_classification_data(data_path)

# --- Simple data augmentation (flips) ---
X_aug, Y_aug = [], []
for i in range(len(X)):
    X_aug.append(X[i])
    Y_aug.append(Y[i])
    X_aug.append(np.flip(X[i], axis=1))  # Horizontal flip
    Y_aug.append(Y[i])
    X_aug.append(np.flip(X[i], axis=0))  # Vertical flip
    Y_aug.append(Y[i])
X = np.array(X_aug)
Y = np.array(Y_aug)


# --- Train/Validation/Test split ---
X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Convert grayscale images to 3-channel (required for EfficientNet)
X_train_rgb = np.repeat(X_train, 3, axis=-1)
X_val_rgb = np.repeat(X_val, 3, axis=-1)
X_test_rgb = np.repeat(X_test, 3, axis=-1)

# --- Define EfficientNetB0 model ---
base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(256, 256, 3))
base_model.trainable = True  # Fine-tune all layers

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.4)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=Adam(1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# --- Compute class weights to handle imbalance ---
weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = {i: weights[i] for i in range(len(weights))}

# callbacks
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),
    ReduceLROnPlateau(patience=5, factor=0.3, min_lr=1e-6)
]

# --- Train the model ---
history = model.fit(
    X_train_rgb, y_train,
    validation_data=(X_val_rgb, y_val),
    epochs=40,
    batch_size=16,
    class_weight=class_weights,
    callbacks=callbacks,
    verbose=2
)

# --- Evaluate on test set ---
y_pred_prob = model.predict(X_test_rgb).flatten()
y_pred = (y_pred_prob > 0.5).astype(int)


print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Benign", "Malignant"]))
print("ðŸ”¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print(f"ðŸ“ˆ AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# confusion matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Benign", "Malignant"])

# --- Plot and save the confusion matrix ---
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - EfficientNetB0")
plt.savefig("efficientnet_confusion_matrix.png", bbox_inches='tight')
plt.show()

from sklearn.metrics import roc_curve, auc

# --- Compute ROC curve and AUC ---
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# --- Plot and save the ROC curve ---
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - EfficientNetB0')
plt.legend(loc="lower right")
plt.grid(True)
plt.savefig("efficientnet_roc_curve.png", bbox_inches='tight')
plt.show()

# --- Assuming 'history' is the output from model.fit ---
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))


# --- Plot and save training history ---
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

plt.suptitle('EfficientNetB0 Training Performance')
plt.savefig("efficientnet_training_plot.png", bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2
from tensorflow.keras.applications.efficientnet import preprocess_input

# --- Last convolutional layer in EfficientNetB0 ---
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model


last_conv_layer_name = "top_conv"

# --- Function to generate Grad-CAM heatmap ---
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # Normalize between 0 and 1
    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)
    return heatmap.numpy()

# --- Function to display Grad-CAM results ---
def display_gradcam(original_img, heatmap, alpha=0.5):
    # resize heatmap to match image size
    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))
    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)
    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)

    overlay = heatmap_color * alpha + original_img
    overlay = np.uint8(overlay)

    # plot
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(original_img)
    plt.title("Original")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(heatmap, cmap='jet')
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(overlay)
    plt.title("Overlay")
    plt.axis("off")
    plt.tight_layout()

    plt.show()


# --- Example: Generate Grad-CAM for sample test images ---
sample_indices = [0, 1, 2, 3, 4]

for idx in sample_indices:
    img = X_test[idx]  # grayscale (256, 256, 1)
    img_rgb = np.repeat(img, 3, axis=-1)
    input_tensor = np.expand_dims(img_rgb, axis=0)
    input_tensor = preprocess_input(input_tensor)

    heatmap = make_gradcam_heatmap(input_tensor, model, last_conv_layer_name)
    original_vis = (img_rgb * 255).astype(np.uint8)
    display_gradcam(original_vis, heatmap)



plt.show()